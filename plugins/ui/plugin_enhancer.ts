/**
 * PersLM Plugin Enhancer
 * 
 * This module provides functionality to analyze and enhance UI plugins by tracking
 * metrics, analyzing lifecycle performance, and suggesting optimizations.
 */

import { UIPluginRegistry, UIPluginBase, UIPluginMetadata, UIPluginLifecycle, UIPluginAnalytics, UIPluginPerformance } from './component_base';
import { performance } from 'perf_hooks';
import chalk from 'chalk';
import fs from 'fs';
import path from 'path';
import { Command } from 'commander';
import { PrismaClient } from '@prisma/client';

// Initialize Prisma client for database operations
const prisma = new PrismaClient();

/**
 * Performance thresholds for various operations (in ms)
 */
const PERFORMANCE_THRESHOLDS = {
  INIT: 150,
  RENDER: 16, // ~60fps
  DESTROY: 50,
  STATE_UPDATE: 10,
  TOTAL_MEMORY: 50 * 1024 * 1024, // 50MB
  EVENT_HANDLER: 5,
};

/**
 * Severity levels for enhancement suggestions
 */
export enum SeverityLevel {
  INFO = 'info',
  WARNING = 'warning',
  ERROR = 'error',
}

/**
 * Types of enhancement suggestions
 */
export enum SuggestionType {
  PERFORMANCE = 'performance',
  MEMORY = 'memory',
  LIFECYCLE = 'lifecycle',
  API_USAGE = 'api_usage',
  TYPE_SAFETY = 'type_safety',
  COMPONENT_STRUCTURE = 'component_structure',
  BEST_PRACTICE = 'best-practice',
  MEMORY_LEAK = 'memory-leak',
  CODE_QUALITY = 'code-quality',
  RENDERING = 'rendering',
  SIDE_EFFECTS = 'side-effects',
  STATE_MANAGEMENT = 'state-management',
  EVENT_HANDLING = 'event-handling',
  ACCESSIBILITY = 'accessibility',
}

/**
 * Model types for comparison
 */
export enum ModelType {
  BASELINE = 'baseline',
  EXPERIMENTAL = 'experimental',
  PRODUCTION = 'production',
  CUSTOM = 'custom'
}

/**
 * Enhancement suggestion interface
 */
export interface EnhancementSuggestion {
  type: SuggestionType;
  severity: SeverityLevel;
  message: string;
  details?: string;
  code?: string;
}

/**
 * Performance metrics for a plugin
 */
export interface PluginPerformanceMetrics {
  initTime?: number;
  destroyTime?: number;
  renderTime?: number;
  stateUpdateTime?: number;
  eventHandlerTime?: number;
  memoryUsage?: number;
  reRenders?: number;
  errorCount?: number;
  apiCallCount?: number;
  unusedState?: string[];
  unusedProps?: string[];
}

/**
 * Plugin analysis report
 */
export interface PluginAnalysis {
  pluginId: string;
  metadata: UIPluginMetadata;
  metrics: PluginPerformanceMetrics;
  suggestions: EnhancementSuggestion[];
  status: 'ok' | 'warning' | 'error';
}

/**
 * Model comparison result
 */
export interface ModelComparisonResult {
  pluginId: string;
  modelType: ModelType;
  baselineMetrics: PluginPerformanceMetrics;
  comparisonMetrics: PluginPerformanceMetrics;
  improvement: {
    initTime?: number;
    renderTime?: number;
    destroyTime?: number;
    memoryUsage?: number;
  };
  timestamp: Date;
}

/**
 * Overall enhancement report
 */
export interface EnhancementReport {
  timestamp: Date;
  pluginCount: number;
  analyses: PluginAnalysis[];
  globalSuggestions: EnhancementSuggestion[];
  modelComparisons?: ModelComparisonResult[];
  summary: {
    ok: number;
    warning: number;
    error: number;
  };
}

/**
 * Options for plugin enhancement
 */
export interface EnhancementOptions {
  outputPath?: string;
  verbose?: boolean;
  fix?: boolean;
  compareWithModel?: ModelType;
  logToDatabase?: boolean;
  thresholds?: Partial<typeof PERFORMANCE_THRESHOLDS>;
}

/**
 * Analyzes a single plugin's performance metrics
 */
function analyzePluginPerformance<TProps = unknown, TSettings = unknown, TState = unknown>(
  plugin: UIPluginBase<TProps, TSettings, TState>
): PluginPerformanceMetrics {
  const metrics: PluginPerformanceMetrics = {
    reRenders: 0,
    errorCount: 0,
    apiCallCount: 0,
    unusedState: [],
    unusedProps: [],
  };

  // Extract performance data if available
  if (plugin.performance) {
    const perfMetrics = plugin.performance.getMetrics();
    metrics.initTime = perfMetrics['init']?.duration;
    metrics.destroyTime = perfMetrics['destroy']?.duration;
    metrics.renderTime = perfMetrics['render']?.duration;
    metrics.stateUpdateTime = perfMetrics['stateUpdate']?.duration;
    metrics.eventHandlerTime = perfMetrics['eventHandler']?.duration;
  }

  // Extract analytics data if available
  if (plugin.analytics) {
    // This would be implementation-specific, assuming the analytics
    // has some method to retrieve usage statistics
    const analyticsData = (plugin.analytics as any)._getInternalStats?.();
    if (analyticsData) {
      metrics.errorCount = analyticsData.errors?.length || 0;
      metrics.apiCallCount = analyticsData.apiCalls?.length || 0;
    }
  }

  // This is a simplified approach - in a real implementation, you would need
  // more sophisticated state/props usage analysis
  const defaultState = plugin.defaultState as Record<string, unknown>;
  const defaultProps = plugin.defaultProps as Record<string, unknown>;

  if (defaultState) {
    // This is a naive check assuming keys not accessed in lifecycle are unused
    // A real implementation would need AST analysis or runtime instrumentation
    metrics.unusedState = Object.keys(defaultState).filter(key => {
      const lifecycleCode = plugin.lifecycle?.toString() || '';
      return !lifecycleCode.includes(`state.${key}`) && 
             !lifecycleCode.includes(`state["${key}"]`);
    });
  }

  if (defaultProps) {
    // Similar naive check for props
    metrics.unusedProps = Object.keys(defaultProps).filter(key => {
      const componentCode = plugin.component.toString();
      return !componentCode.includes(`props.${key}`) && 
             !componentCode.includes(`props["${key}"]`);
    });
  }

  return metrics;
}

/**
 * Generates enhancement suggestions based on metrics
 */
function generateSuggestions(
  pluginId: string,
  metrics: PluginPerformanceMetrics,
  thresholds = PERFORMANCE_THRESHOLDS
): EnhancementSuggestion[] {
  const suggestions: EnhancementSuggestion[] = [];

  // Check init time
  if (metrics.initTime && metrics.initTime > thresholds.INIT) {
    suggestions.push({
      type: SuggestionType.PERFORMANCE,
      severity: SeverityLevel.WARNING,
      message: `Slow initialization (${metrics.initTime}ms)`,
      details: 'Consider lazy loading dependencies or deferring initialization work',
      code: `// Before
init: () => {
  loadAllDependencies();
  processAllData();
}

// After
init: () => {
  // Only load critical dependencies
  loadCriticalDependencies();
  
  // Defer other work
  setTimeout(() => {
    loadOptionalDependencies();
    processAllData();
  }, 0);
}`
    });
  }

  // Check render time
  if (metrics.renderTime && metrics.renderTime > thresholds.RENDER) {
    suggestions.push({
      type: SuggestionType.PERFORMANCE,
      severity: SeverityLevel.WARNING,
      message: `Slow rendering (${metrics.renderTime}ms)`,
      details: 'Consider optimizing render method with memoization or splitting into smaller components',
      code: `// Add React.memo to prevent unnecessary re-renders
const MemoizedComponent = React.memo(YourComponent);

// Or use useMemo for expensive computations
const expensiveValue = useMemo(() => computeExpensiveValue(dep1, dep2), [dep1, dep2]);`
    });
  }

  // Check destroy time
  if (metrics.destroyTime && metrics.destroyTime > thresholds.DESTROY) {
    suggestions.push({
      type: SuggestionType.LIFECYCLE,
      severity: SeverityLevel.WARNING,
      message: `Slow cleanup (${metrics.destroyTime}ms)`,
      details: 'Cleanup operations taking too long may block UI when component unmounts',
      code: `// Consider breaking down cleanup operations
destroy: () => {
  // Prioritize resource cleanup
  cleanupCriticalResources();
  
  // Defer non-critical cleanup
  queueMicrotask(() => {
    finishRemainingCleanup();
  });
}`
    });
  }

  // Check for unused state
  if (metrics.unusedState && metrics.unusedState.length > 0) {
    suggestions.push({
      type: SuggestionType.MEMORY,
      severity: SeverityLevel.INFO,
      message: `Unused state fields detected: ${metrics.unusedState.join(', ')}`,
      details: 'Remove unused state to improve memory usage and prevent unnecessary updates',
    });
  }

  // Check for unused props
  if (metrics.unusedProps && metrics.unusedProps.length > 0) {
    suggestions.push({
      type: SuggestionType.API_USAGE,
      severity: SeverityLevel.INFO,
      message: `Unused props detected: ${metrics.unusedProps.join(', ')}`,
      details: 'Remove unused props to improve clarity and performance',
    });
  }

  // Check error count
  if (metrics.errorCount && metrics.errorCount > 0) {
    suggestions.push({
      type: SuggestionType.COMPONENT_STRUCTURE,
      severity: SeverityLevel.ERROR,
      message: `Plugin reported ${metrics.errorCount} errors during operation`,
      details: 'Errors may indicate broken functionality or edge cases not handled correctly',
    });
  }

  // Add more checks as needed...

  return suggestions;
}

/**
 * Wraps plugin lifecycle methods with performance monitoring
 */
function enhancePluginLifecycleMethods<TProps = unknown, TSettings = unknown, TState = unknown>(
  plugin: UIPluginBase<TProps, TSettings, TState>
): void {
  if (!plugin.lifecycle) return;
  
  // Create performance hooks if not present
  if (!plugin.performance) {
    plugin.performance = {
      measure: (id: string, startTime?: number) => {
        const end = performance.now();
        const start = startTime || 0;
        return end - start;
      },
      markStart: (id: string) => {
        (plugin.performance as any)._marks = (plugin.performance as any)._marks || {};
        (plugin.performance as any)._marks[id] = performance.now();
      },
      markEnd: (id: string) => {
        if (!(plugin.performance as any)._marks?.[id]) return 0;
        const duration = performance.now() - (plugin.performance as any)._marks[id];
        
        (plugin.performance as any)._metrics = (plugin.performance as any)._metrics || {};
        (plugin.performance as any)._metrics[id] = {
          duration,
          timestamp: Date.now()
        };
        
        return duration;
      },
      getMetrics: () => {
        return (plugin.performance as any)._metrics || {};
      }
    };
  }
  
  // Create analytics if not present
  if (!plugin.analytics) {
    plugin.analytics = {
      trackInteraction: () => {},
      trackError: (error: Error) => {
        (plugin.analytics as any)._errors = (plugin.analytics as any)._errors || [];
        (plugin.analytics as any)._errors.push({
          message: error.message,
          stack: error.stack,
          timestamp: Date.now()
        });
      },
      trackUsage: () => {},
    };
    
    // Add internal stats collector
    (plugin.analytics as any)._getInternalStats = () => {
      return {
        errors: (plugin.analytics as any)._errors || [],
        apiCalls: (plugin.analytics as any)._apiCalls || [],
        interactions: (plugin.analytics as any)._interactions || []
      };
    };
  }
  
  // Wrap lifecycle methods with performance tracking
  const originalLifecycle = { ...plugin.lifecycle };
  
  if (originalLifecycle.init) {
    plugin.lifecycle.init = function() {
      plugin.performance?.markStart('init');
      try {
        const result = originalLifecycle.init?.apply(this, arguments as any);
        return result;
      } catch (error) {
        plugin.analytics?.trackError(error as Error);
        throw error;
      } finally {
        plugin.performance?.markEnd('init');
      }
    };
  }
  
  if (originalLifecycle.destroy) {
    plugin.lifecycle.destroy = function() {
      plugin.performance?.markStart('destroy');
      try {
        const result = originalLifecycle.destroy?.apply(this, arguments as any);
        return result;
      } catch (error) {
        plugin.analytics?.trackError(error as Error);
        throw error;
      } finally {
        plugin.performance?.markEnd('destroy');
      }
    };
  }
  
  if (originalLifecycle.setState) {
    plugin.lifecycle.setState = function(state) {
      plugin.performance?.markStart('stateUpdate');
      try {
        return originalLifecycle.setState?.apply(this, arguments as any);
      } catch (error) {
        plugin.analytics?.trackError(error as Error);
        throw error;
      } finally {
        plugin.performance?.markEnd('stateUpdate');
      }
    };
  }
  
  // Wrap component with performance tracking
  const originalComponent = plugin.component;
  plugin.component = function(props) {
    plugin.performance?.markStart('render');
    try {
      (plugin as any)._renderCount = ((plugin as any)._renderCount || 0) + 1;
      return originalComponent(props);
    } catch (error) {
      plugin.analytics?.trackError(error as Error);
      throw error;
    } finally {
      plugin.performance?.markEnd('render');
    }
  } as typeof plugin.component;
}

/**
 * Checks if a plugin needs enhancements based on its current state
 */
function pluginNeedsEnhancement<TProps = unknown, TSettings = unknown, TState = unknown>(
  plugin: UIPluginBase<TProps, TSettings, TState>
): boolean {
  // Plugin already has internal performance tracking marker
  if ((plugin as any)._enhanced) {
    return false;
  }
  
  // Check if plugin has basic requirements for enhancement
  return (
    plugin.metadata?.id &&
    typeof plugin.component === 'function' &&
    !!(plugin.lifecycle || plugin.defaultState || plugin.defaultProps)
  );
}

/**
 * Analyzes a registry of plugins and enhances them with performance tracking
 */
export function analyzeAndEnhancePlugins(
  pluginRegistry: UIPluginRegistry,
  options: EnhancementOptions = {}
): EnhancementReport {
  const startTime = performance.now();
  const timestamp = new Date();
  const analyses: PluginAnalysis[] = [];
  const globalSuggestions: EnhancementSuggestion[] = [];
  const mergedThresholds = { ...PERFORMANCE_THRESHOLDS, ...options.thresholds };
  
  // Get all plugins from registry
  const plugins = pluginRegistry.getAllPlugins();
  
  // Process each plugin
  for (const plugin of plugins) {
    try {
      // Check if plugin needs enhancement
      if (pluginNeedsEnhancement(plugin) && options.fix !== false) {
        enhancePluginLifecycleMethods(plugin);
        (plugin as any)._enhanced = timestamp;
      }
      
      // Analyze plugin performance
      const metrics = analyzePluginPerformance(plugin);
      
      // Generate suggestions
      const suggestions = generateSuggestions(plugin.metadata.id, metrics, mergedThresholds);
      
      // Determine status
      let status: 'ok' | 'warning' | 'error' = 'ok';
      if (suggestions.some(s => s.severity === SeverityLevel.ERROR)) {
        status = 'error';
      } else if (suggestions.some(s => s.severity === SeverityLevel.WARNING)) {
        status = 'warning';
      }
      
      // Create analysis
      analyses.push({
        pluginId: plugin.metadata.id,
        metadata: plugin.metadata,
        metrics,
        suggestions,
        status
      });
    } catch (error) {
      console.error(`Error analyzing plugin ${plugin.metadata?.id || 'unknown'}:`, error);
      
      // Add to global suggestions
      globalSuggestions.push({
        type: SuggestionType.COMPONENT_STRUCTURE,
        severity: SeverityLevel.ERROR,
        message: `Failed to analyze plugin ${plugin.metadata?.id || 'unknown'}`,
        details: error instanceof Error ? error.message : String(error)
      });
    }
  }
  
  // Generate summary
  const summary = {
    ok: analyses.filter(a => a.status === 'ok').length,
    warning: analyses.filter(a => a.status === 'warning').length,
    error: analyses.filter(a => a.status === 'error').length
  };
  
  // Add global suggestions about plugin ecosystem
  if (analyses.length === 0) {
    globalSuggestions.push({
      type: SuggestionType.API_USAGE,
      severity: SeverityLevel.INFO,
      message: 'No plugins found in registry',
      details: 'Ensure plugins are properly registered before analysis'
    });
  }
  
  if (analyses.length > 20) {
    globalSuggestions.push({
      type: SuggestionType.PERFORMANCE,
      severity: SeverityLevel.WARNING,
      message: `Large number of plugins detected (${analyses.length})`,
      details: 'Consider consolidating plugins or lazy-loading non-critical plugins'
    });
  }
  
  // Create report
  const report: EnhancementReport = {
    timestamp,
    pluginCount: plugins.length,
    analyses,
    globalSuggestions,
    summary
  };
  
  // Output report if requested
  if (options.outputPath) {
    const outputDir = path.dirname(options.outputPath);
    if (!fs.existsSync(outputDir)) {
      fs.mkdirSync(outputDir, { recursive: true });
    }
    fs.writeFileSync(
      options.outputPath,
      JSON.stringify(report, null, 2),
      'utf-8'
    );
  }
  
  return report;
}

/**
 * Formats and prints a report to the console
 */
export function printReport(report: EnhancementReport, verbose = false): void {
  console.log(chalk.bold(`\nPersLM Plugin Enhancement Report (${report.timestamp.toISOString()})`));
  console.log(`Analyzed ${report.pluginCount} plugins\n`);
  
  // Summary
  console.log(chalk.bold('Summary:'));
  console.log(`âœ… OK: ${report.summary.ok}`);
  console.log(`âš ï¸ Warnings: ${report.summary.warning}`);
  console.log(`âŒ Errors: ${report.summary.error}\n`);
  
  // Global suggestions
  if (report.globalSuggestions.length > 0) {
    console.log(chalk.bold('Global Suggestions:'));
    for (const suggestion of report.globalSuggestions) {
      const icon = suggestion.severity === SeverityLevel.ERROR ? 'âŒ' :
                   suggestion.severity === SeverityLevel.WARNING ? 'âš ï¸' : 'â„¹ï¸';
      console.log(`${icon} ${suggestion.message}`);
      if (verbose && suggestion.details) {
        console.log(`   ${chalk.gray(suggestion.details)}`);
      }
    }
    console.log('');
  }
  
  // Plugin analyses
  console.log(chalk.bold('Plugin Analyses:'));
  for (const analysis of report.analyses) {
    const icon = analysis.status === 'error' ? 'âŒ' :
                 analysis.status === 'warning' ? 'âš ï¸' : 'âœ…';
    
    console.log(`${icon} Plugin: ${chalk.cyan(analysis.metadata.name)} (${analysis.pluginId})`);
    
    if (verbose) {
      if (analysis.metrics.initTime) {
        console.log(`   Init time: ${analysis.metrics.initTime.toFixed(2)}ms`);
      }
      if (analysis.metrics.renderTime) {
        console.log(`   Render time: ${analysis.metrics.renderTime.toFixed(2)}ms`);
      }
      if (analysis.metrics.errorCount && analysis.metrics.errorCount > 0) {
        console.log(`   Errors: ${analysis.metrics.errorCount}`);
      }
    }
    
    if (analysis.suggestions.length > 0) {
      for (const suggestion of analysis.suggestions) {
        const suggestionIcon = suggestion.severity === SeverityLevel.ERROR ? 'âŒ' :
                              suggestion.severity === SeverityLevel.WARNING ? 'âš ï¸' : 'â„¹ï¸';
        console.log(`   ${suggestionIcon} ${suggestion.message}`);
        if (verbose && suggestion.details) {
          console.log(`      ${chalk.gray(suggestion.details)}`);
        }
      }
    } else if (analysis.status === 'ok') {
      console.log(`   ${chalk.green('All checks passed')}`);
    }
    
    console.log('');
  }
}

/**
 * CLI entry point
 */
if (require.main === module) {
  const main = async () => {
    const program = new Command();
    
    program
      .name('plugin-enhancer')
      .description('PersLM Plugin Enhancement Tool')
      .version('1.0.0');
    
    program
      .command('analyze')
      .description('Analyze plugins and generate a report')
      .option('-o, --output <path>', 'Output path for the report JSON', './plugin-report.json')
      .option('-d, --database', 'Log results to database', false)
      .action(async (options) => {
        console.log(chalk.blue.bold('PersLM Plugin Enhancer'));
        console.log(chalk.cyan('Analyzing plugins...'));
        
        try {
          // This is a mock implementation - in a real scenario you would:
          // 1. Import the actual UIPluginRegistry
          // 2. Get all the registered plugins
          // 3. Run the enhancer on them
          
          // Mock registry
          const mockRegistry = {
            getAllPlugins: () => ({
              'plugin1': {
                metadata: { name: 'Plugin 1', version: '1.0.0' },
                initialize: () => { console.log('Init Plugin 1'); },
                destroy: () => { console.log('Destroy Plugin 1'); },
                toString: () => 'class Plugin1 implements UIPluginBase { /* implementation */ }'
              },
              'plugin2': {
                metadata: { name: 'Plugin 2' },
                initialize: () => { console.log('Init Plugin 2'); },
                // Missing destroy method
                toString: () => 'class Plugin2 implements UIPluginBase { /* partial implementation */ }'
              },
              'plugin3': {
                metadata: { name: 'Plugin 3', version: '0.5.0' },
                initialize: () => { console.log('Init Plugin 3'); },
                destroy: () => { console.log('Destroy Plugin 3'); },
                toString: () => 'class Plugin3 implements UIPluginBase { /* implementation with any */ }: any'
              }
            })
          };
          
          const enhancer = new PluginEnhancer(mockRegistry as any, {
            logToDatabase: options.database
          });
          
          const report = await enhancer.analyze();
          
          // Print summary
          console.log(chalk.green.bold('\nAnalysis Summary:'));
          console.log(chalk.white(`Total plugins: ${report.pluginCount}`));
          console.log(chalk.green(`OK: ${report.summary.ok}`));
          console.log(chalk.yellow(`Warnings: ${report.summary.warning}`));
          console.log(chalk.red(`Errors: ${report.summary.error}`));
          
          // Save report
          const outputPath = options.output || './plugin-report.json';
          const outputDir = path.dirname(outputPath);
          if (!fs.existsSync(outputDir)) {
            fs.mkdirSync(outputDir, { recursive: true });
          }
          fs.writeFileSync(
            outputPath, 
            JSON.stringify(report, null, 2),
            'utf-8'
          );
          
          console.log(chalk.green.bold('\nAnalysis completed successfully!'));
          console.log(chalk.white(`Report saved to: ${outputPath}`));
          console.log(chalk.cyan('Run "enhance-plugins enhance" to apply recommended improvements.'));
        } catch (error) {
          console.error(chalk.red('Error during analysis:'), error);
          process.exit(1);
        }
      });
    
    program
      .command('enhance')
      .description('Apply enhancements to plugins based on analysis')
      .option('-r, --report <path>', 'Path to the report file', './plugin-report.json')
      .option('-d, --database', 'Log results to database', false)
      .action(async (options) => {
        console.log(chalk.blue.bold('PersLM Plugin Enhancer'));
        console.log(chalk.cyan('Applying enhancements...'));
        
        try {
          // This is a mock implementation - in a real scenario you would:
          // 1. Load the report from the file
          // 2. Recreate the enhancer with the actual registry
          // 3. Apply the enhancements
          
          // Load report from file
          if (!fs.existsSync(options.report)) {
            console.error(chalk.red(`Report file not found: ${options.report}`));
            console.log(chalk.yellow('Run "enhance-plugins analyze" first to generate a report.'));
            process.exit(1);
          }
          
          const reportData = JSON.parse(fs.readFileSync(options.report, 'utf-8'));
          
          // Mock registry
          const mockRegistry = {
            getAllPlugins: () => ({
              // This would be populated with real plugins in a real implementation
            })
          };
          
          const enhancer = new PluginEnhancer(mockRegistry as any, {
            logToDatabase: options.database
          });
          
          // Set the loaded report
          enhancer.report = reportData;
          
          // Apply enhancements
          enhancer.applyEnhancements();
          
        } catch (error) {
          console.error(chalk.red('Error during enhancement:'), error);
          process.exit(1);
        }
      });
    
    program
      .command('compare')
      .description('Compare plugin performance against different models')
      .option('-r, --report <path>', 'Path to the report file', './plugin-report.json')
      .option('-m, --model <type>', 'Model to compare against', 'experimental')
      .option('-d, --database', 'Log results to database', false)
      .action(async (options) => {
        console.log(chalk.blue.bold('PersLM Plugin Enhancer'));
        console.log(chalk.cyan(`Comparing against ${options.model} model...`));
        
        try {
          // Load report from file
          if (!fs.existsSync(options.report)) {
            console.error(chalk.red(`Report file not found: ${options.report}`));
            console.log(chalk.yellow('Run "enhance-plugins analyze" first to generate a report.'));
            process.exit(1);
          }
          
          const reportData = JSON.parse(fs.readFileSync(options.report, 'utf-8'));
          
          // Mock registry
          const mockRegistry = {
            getAllPlugins: () => ({
              // This would be populated with real plugins in a real implementation
            })
          };
          
          const enhancer = new PluginEnhancer(mockRegistry as any, {
            logToDatabase: options.database
          });
          
          // Set the loaded report
          enhancer.report = reportData;
          
          // Compare with specified model
          const modelType = options.model as ModelType;
          await enhancer.compareWithModel(modelType);
          
          console.log(chalk.green.bold('\nComparison completed successfully!'));
          
        } catch (error) {
          console.error(chalk.red('Error during comparison:'), error);
          process.exit(1);
        }
      });
    
    program
      .command('train')
      .description('Run autonomous training against different models')
      .option('-i, --iterations <number>', 'Number of training iterations', '5')
      .option('-b, --baseline <model>', 'Baseline model', 'baseline')
      .option('-e, --experimental <model>', 'Experimental model', 'experimental')
      .option('-d, --database', 'Log results to database', true)
      .action(async (options) => {
        console.log(chalk.blue.bold('PersLM Plugin Enhancer'));
        console.log(chalk.cyan('Starting autonomous training...'));
        
        try {
          // Create mock registry
          const mockRegistry = {
            getAllPlugins: () => ({
              'plugin1': {
                metadata: { id: 'plugin1', name: 'Plugin 1', version: '1.0.0' },
                initialize: () => { console.log('Init Plugin 1'); },
                destroy: () => { console.log('Destroy Plugin 1'); },
                toString: () => 'class Plugin1 implements UIPluginBase { /* implementation */ }'
              },
              'plugin2': {
                metadata: { id: 'plugin2', name: 'Plugin 2' },
                initialize: () => { console.log('Init Plugin 2'); },
                toString: () => 'class Plugin2 implements UIPluginBase { /* partial implementation */ }'
              },
              'plugin3': {
                metadata: { id: 'plugin3', name: 'Plugin 3', version: '0.5.0' },
                initialize: () => { console.log('Init Plugin 3'); },
                destroy: () => { console.log('Destroy Plugin 3'); },
                toString: () => 'class Plugin3 implements UIPluginBase { /* implementation with any */ }: any'
              }
            })
          };
          
          // Create the enhancer
          const enhancer = new PluginEnhancer(mockRegistry as any, {
            logToDatabase: options.database
          });
          
          // Run initial analysis
          console.log(chalk.cyan('Running initial analysis...'));
          const report = await enhancer.analyze();
          
          // Create training session
          const trainingSession = await prisma.trainingSession.create({
            data: {
              baselineModel: options.baseline,
              experimentalModel: options.experimental,
              iterations: parseInt(options.iterations),
              report: {
                connect: {
                  id: 1 // In a real implementation, use the actual report ID
                }
              }
            }
          });
          
          console.log(chalk.green(`Training session created with ID: ${trainingSession.id}`));
          
          // Run the specified number of iterations
          const iterations = parseInt(options.iterations);
          console.log(chalk.cyan(`Running ${iterations} training iterations...`));
          
          for (let i = 0; i < iterations; i++) {
            console.log(chalk.cyan(`\nIteration ${i + 1}/${iterations}`));
            
            // Compare with baseline model
            console.log(chalk.cyan(`Comparing with ${options.baseline} model...`));
            const baselineResults = await enhancer.compareWithModel(options.baseline as ModelType);
            
            // Compare with experimental model
            console.log(chalk.cyan(`Comparing with ${options.experimental} model...`));
            const experimentalResults = await enhancer.compareWithModel(options.experimental as ModelType);
            
            // Analyze results and extract learning
            const learnings = await extractLearnings(baselineResults, experimentalResults);
            
            // Store learnings in database
            for (const learning of learnings) {
              await prisma.trainingLearning.create({
                data: {
                  category: learning.category,
                  description: learning.description,
                  confidence: learning.confidence,
                  appliedToPlugins: learning.plugins,
                  trainingSession: {
                    connect: {
                      id: trainingSession.id
                    }
                  }
                }
              });
            }
            
            console.log(chalk.green(`âœ… Iteration ${i + 1} completed with ${learnings.length} learnings`));
            
            // Apply learnings to improve future iterations
            if (i < iterations - 1) {
              console.log(chalk.cyan('Applying learnings to improve next iteration...'));
              // In a real implementation, you would modify the plugins or models based on learnings
              await new Promise(resolve => setTimeout(resolve, 1000)); // Simulate processing time
            }
          }
          
          // Update training session status
          await prisma.trainingSession.update({
            where: {
              id: trainingSession.id
            },
            data: {
              endTime: new Date(),
              status: 'completed'
            }
          });
          
          console.log(chalk.green.bold('\nAutonomous training completed successfully!'));
          console.log(chalk.white(`Training session ID: ${trainingSession.id}`));
          console.log(chalk.cyan('Run "enhance-plugins report --training" to view training results.'));
          
        } catch (error) {
          console.error(chalk.red('Error during training:'), error);
          process.exit(1);
        }
      });
    
    program
      .command('view')
      .description('View the latest report in the console')
      .option('-r, --report <path>', 'Path to the report file', './plugin-report.json')
      .option('-t, --training', 'View training session results', false)
      .action(async (options) => {
        console.log(chalk.blue.bold('PersLM Plugin Enhancer'));
        
        try {
          if (options.training) {
            // Display training session data from database
            const trainingSessions = await prisma.trainingSession.findMany({
              orderBy: {
                startTime: 'desc'
              },
              take: 1,
              include: {
                learnings: true,
                results: true
              }
            });
            
            if (trainingSessions.length === 0) {
              console.log(chalk.yellow('No training sessions found.'));
              console.log(chalk.cyan('Run "enhance-plugins train" to generate training data.'));
              return;
            }
            
            const session = trainingSessions[0];
            
            console.log(chalk.green.bold('\nTraining Session Summary:'));
            console.log(chalk.white(`Session ID: ${session.id}`));
            console.log(chalk.white(`Start time: ${session.startTime.toLocaleString()}`));
            console.log(chalk.white(`Status: ${session.status}`));
            console.log(chalk.white(`Baseline model: ${session.baselineModel}`));
            console.log(chalk.white(`Experimental model: ${session.experimentalModel}`));
            console.log(chalk.white(`Iterations: ${session.iterations}`));
            
            if (session.learnings.length > 0) {
              console.log(chalk.cyan.bold('\nLearnings:'));
              session.learnings.forEach((learning, index) => {
                console.log(chalk.white.bold(`\n${index + 1}. ${learning.category} (${(learning.confidence * 100).toFixed(1)}% confidence):`));
                console.log(chalk.white(`   ${learning.description}`));
                console.log(chalk.gray(`   Applied to: ${learning.appliedToPlugins.join(', ')}`));
              });
            } else {
              console.log(chalk.yellow('\nNo learnings recorded for this session.'));
            }
            
            console.log(chalk.cyan.bold('\nModel Comparison Results:'));
            const pluginResults = new Map();
            
            // Group results by plugin
            for (const result of session.results) {
              if (!pluginResults.has(result.pluginId)) {
                pluginResults.set(result.pluginId, []);
              }
              pluginResults.get(result.pluginId).push(result);
            }
            
            for (const [pluginId, results] of pluginResults.entries()) {
              console.log(chalk.white.bold(`\n${pluginId}:`));
              
              for (const result of results) {
                console.log(chalk.white(`  ${result.modelType} model:`));
                
                if (result.initTimeImprovement) {
                  const color = result.initTimeImprovement > 0 ? chalk.green : chalk.red;
                  console.log(`    Init time: ${color(`${Math.abs(result.initTimeImprovement).toFixed(1)}% ${result.initTimeImprovement > 0 ? 'faster' : 'slower'}`)}`);
                }
                
                if (result.renderTimeImprovement) {
                  const color = result.renderTimeImprovement > 0 ? chalk.green : chalk.red;
                  console.log(`    Render time: ${color(`${Math.abs(result.renderTimeImprovement).toFixed(1)}% ${result.renderTimeImprovement > 0 ? 'faster' : 'slower'}`)}`);
                }
              }
            }
            
          } else {
            // Load report from file
            if (!fs.existsSync(options.report)) {
              console.error(chalk.red(`Report file not found: ${options.report}`));
              console.log(chalk.yellow('Run "enhance-plugins analyze" first to generate a report.'));
              process.exit(1);
            }
            
            const report = JSON.parse(fs.readFileSync(options.report, 'utf-8'));
            
            // Display report summary
            console.log(chalk.green.bold('\nReport Summary:'));
            console.log(chalk.white(`Report generated: ${new Date(report.timestamp).toLocaleString()}`));
            console.log(chalk.white(`Total plugins: ${report.pluginCount}`));
            console.log(chalk.green(`OK: ${report.summary.ok}`));
            console.log(chalk.yellow(`Warnings: ${report.summary.warning}`));
            console.log(chalk.red(`Errors: ${report.summary.error}`));
            
            // Display global suggestions
            if (report.globalSuggestions.length > 0) {
              console.log(chalk.cyan.bold('\nGlobal Suggestions:'));
              report.globalSuggestions.forEach((suggestion, index) => {
                const severityColor = 
                  suggestion.severity === SeverityLevel.ERROR ? chalk.red :
                  suggestion.severity === SeverityLevel.WARNING ? chalk.yellow : 
                  chalk.blue;
                
                console.log(severityColor(`${index + 1}. [${suggestion.severity.toUpperCase()}] ${suggestion.message}`));
                if (suggestion.details) {
                  console.log(chalk.white(`   ${suggestion.details}`));
                }
              });
            }
            
            // Display plugin issues
            console.log(chalk.cyan.bold('\nPlugin Issues:'));
            report.analyses
              .filter(analysis => analysis.status !== 'ok')
              .forEach(analysis => {
                console.log(chalk.white.bold(`\n${analysis.metadata.name || analysis.pluginId}:`));
                analysis.suggestions.forEach((suggestion, index) => {
                  const severityColor = 
                    suggestion.severity === SeverityLevel.ERROR ? chalk.red :
                    suggestion.severity === SeverityLevel.WARNING ? chalk.yellow : 
                    chalk.blue;
                  
                  console.log(severityColor(`${index + 1}. [${suggestion.severity.toUpperCase()}] ${suggestion.message}`));
                  if (suggestion.details) {
                    console.log(chalk.white(`   ${suggestion.details}`));
                  }
                });
              });
            
            // Suggest next steps
            console.log(chalk.green.bold('\nRecommended Actions:'));
            if (report.summary.error > 0) {
              console.log(chalk.red('â€¢ Address all errors before deploying to production'));
            }
            if (report.summary.warning > 0) {
              console.log(chalk.yellow('â€¢ Review warnings to improve plugin quality'));
            }
            console.log(chalk.white('â€¢ Run "enhance-plugins enhance" to apply automatic fixes'));
            console.log(chalk.white('â€¢ Run "enhance-plugins train" to start autonomous model testing'));
          }
          
        } catch (error) {
          console.error(chalk.red('Error viewing report:'), error);
          process.exit(1);
        }
      });
    
    // If no arguments, show help
    if (process.argv.length <= 2) {
      program.help();
      return;
    }
    
    await program.parseAsync();
  };
  
  main().catch(error => {
    console.error('Failed to run the plugin enhancer:', error);
    process.exit(1);
  });
}

/**
 * Extract learnings from model comparison results
 */
async function extractLearnings(
  baselineResults: ModelComparisonResult[],
  experimentalResults: ModelComparisonResult[]
): Promise<Array<{
  category: string;
  description: string;
  confidence: number;
  plugins: string[];
}>> {
  const learnings: Array<{
    category: string;
    description: string;
    confidence: number;
    plugins: string[];
  }> = [];
  
  // This is a simplified mock implementation
  // In a real implementation, you would perform statistical analysis
  // and pattern recognition to extract meaningful learnings
  
  // Map results by plugin ID
  const resultsByPlugin = new Map<string, {
    baseline: ModelComparisonResult;
    experimental: ModelComparisonResult;
  }>();
  
  for (const result of baselineResults) {
    resultsByPlugin.set(result.pluginId, {
      baseline: result,
      experimental: experimentalResults.find(r => r.pluginId === result.pluginId)!
    });
  }
  
  // Check for initialization time improvements
  const initTimePlugins = Array.from(resultsByPlugin.entries())
    .filter(([_, results]) => 
      results.experimental.improvement.initTime && 
      results.experimental.improvement.initTime > 15)
    .map(([pluginId, _]) => pluginId);
  
  if (initTimePlugins.length > 0) {
    learnings.push({
      category: 'performance',
      description: 'Experimental model significantly improves initialization times',
      confidence: 0.8,
      plugins: initTimePlugins
    });
  }
  
  // Check for render time improvements
  const renderTimePlugins = Array.from(resultsByPlugin.entries())
    .filter(([_, results]) => 
      results.experimental.improvement.renderTime && 
      results.experimental.improvement.renderTime > 10)
    .map(([pluginId, _]) => pluginId);
  
  if (renderTimePlugins.length > 0) {
    learnings.push({
      category: 'performance',
      description: 'Experimental model renders components more efficiently',
      confidence: 0.75,
      plugins: renderTimePlugins
    });
  }
  
  // If no specific learnings were extracted, add a generic one
  if (learnings.length === 0) {
    const allPlugins = Array.from(resultsByPlugin.keys());
    learnings.push({
      category: 'general',
      description: 'No significant performance differences detected between models',
      confidence: 0.6,
      plugins: allPlugins
    });
  }
  
  return learnings;
}

// Run CLI when executed directly
if (require.main === module) {
  main();
}

/**
 * Class for enhancing and comparing plugins
 */
export class PluginEnhancer {
  private registry: UIPluginRegistry;
  private options: EnhancementOptions;
  public report: EnhancementReport | null = null;

  constructor(registry: UIPluginRegistry, options: EnhancementOptions = {}) {
    this.registry = registry;
    this.options = {
      verbose: false,
      fix: true,
      logToDatabase: true,
      ...options
    };
  }

  /**
   * Analyze all plugins in the registry
   */
  public async analyze(): Promise<EnhancementReport> {
    this.report = analyzeAndEnhancePlugins(this.registry, this.options);
    
    if (this.options.logToDatabase) {
      await this.logReportToDatabase(this.report);
    }
    
    if (this.options.compareWithModel) {
      await this.compareWithModel(this.options.compareWithModel);
    }
    
    return this.report;
  }

  /**
   * Apply enhancements to all plugins based on analysis
   */
  public applyEnhancements(): void {
    if (!this.report) {
      throw new Error('Must run analyze() before applying enhancements');
    }
    
    console.log(chalk.blue.bold('Applying plugin enhancements...'));
    
    const plugins = this.registry.getAllPlugins();
    
    for (const plugin of plugins) {
      const analysis = this.report.analyses.find(a => a.pluginId === plugin.metadata.id);
      
      if (!analysis) continue;
      
      // Skip plugins that are working well
      if (analysis.status === 'ok' && analysis.suggestions.length === 0) {
        console.log(chalk.green(`âœ… ${plugin.metadata.name || plugin.metadata.id}: No enhancements needed`));
        continue;
      }
      
      console.log(chalk.yellow(`ðŸ“ Enhancing ${plugin.metadata.name || plugin.metadata.id}...`));
      
      // Apply performance monitoring if not already enhanced
      if (pluginNeedsEnhancement(plugin)) {
        enhancePluginLifecycleMethods(plugin);
        console.log(chalk.green('  âœ“ Added performance monitoring'));
      }
      
      // Apply memoization for slow rendering components
      if (analysis.metrics.renderTime && analysis.metrics.renderTime > PERFORMANCE_THRESHOLDS.RENDER) {
        this.applyMemoization(plugin);
        console.log(chalk.green('  âœ“ Applied memoization to component'));
      }
      
      // Apply throttling for frequent state updates
      if (analysis.metrics.stateUpdateTime && analysis.metrics.stateUpdateTime > PERFORMANCE_THRESHOLDS.STATE_UPDATE) {
        this.applyThrottling(plugin);
        console.log(chalk.green('  âœ“ Applied throttling to state updates'));
      }
      
      console.log(chalk.green(`âœ… Finished enhancing ${plugin.metadata.name || plugin.metadata.id}`));
    }
    
    console.log(chalk.green.bold('\nEnhancement process completed!'));
  }

  /**
   * Apply memoization to a plugin component
   */
  private applyMemoization<TProps = unknown, TSettings = unknown, TState = unknown>(
    plugin: UIPluginBase<TProps, TSettings, TState>
  ): void {
    const originalComponent = plugin.component;
    
    // Simple memoization implementation - in a real-world scenario,
    // you would use React.memo or a more sophisticated approach
    const memoizedResults = new Map();
    
    plugin.component = function(props: TProps) {
      // Use JSON.stringify for simple prop comparison
      // In a real implementation, you would use a more efficient
      // and accurate comparison method
      const propsKey = JSON.stringify(props);
      
      if (!memoizedResults.has(propsKey)) {
        const result = originalComponent(props);
        memoizedResults.set(propsKey, result);
        
        // Prevent unlimited cache growth
        if (memoizedResults.size > 20) {
          // Remove oldest entry (this is a simple approach)
          const firstKey = memoizedResults.keys().next().value;
          memoizedResults.delete(firstKey);
        }
      }
      
      return memoizedResults.get(propsKey);
    } as typeof plugin.component;
  }

  /**
   * Apply throttling to plugin state updates
   */
  private applyThrottling<TProps = unknown, TSettings = unknown, TState = unknown>(
    plugin: UIPluginBase<TProps, TSettings, TState>
  ): void {
    if (!plugin.lifecycle || !plugin.lifecycle.setState) return;
    
    const originalSetState = plugin.lifecycle.setState;
    let lastUpdateTime = 0;
    const THROTTLE_THRESHOLD = 50; // ms
    
    plugin.lifecycle.setState = function(state: Partial<TState>) {
      const now = performance.now();
      
      // If we've updated recently, throttle the update
      if (now - lastUpdateTime < THROTTLE_THRESHOLD) {
        // Schedule state update for later
        setTimeout(() => {
          originalSetState.call(this, state);
          lastUpdateTime = performance.now();
        }, THROTTLE_THRESHOLD);
      } else {
        // Update immediately
        originalSetState.call(this, state);
        lastUpdateTime = now;
      }
    };
  }

  /**
   * Compare plugin performance against another model
   */
  public async compareWithModel(modelType: ModelType): Promise<ModelComparisonResult[]> {
    if (!this.report) {
      throw new Error('Must run analyze() before comparing models');
    }
    
    console.log(chalk.blue.bold(`Comparing against ${modelType} model...`));
    
    // In a real implementation, you would:
    // 1. Load a different model implementation
    // 2. Run the same plugins against it
    // 3. Compare performance metrics
    
    // This is a mock implementation for demonstration
    const comparisonResults: ModelComparisonResult[] = [];
    
    for (const analysis of this.report.analyses) {
      // Mock the comparison model metrics - in a real implementation,
      // you would actually measure these by running the plugin with the comparison model
      const baselineMetrics = analysis.metrics;
      
      // Generate mock comparison metrics with some random variation
      const comparisonMetrics: PluginPerformanceMetrics = {
        initTime: baselineMetrics.initTime ? baselineMetrics.initTime * (0.7 + Math.random() * 0.6) : undefined,
        renderTime: baselineMetrics.renderTime ? baselineMetrics.renderTime * (0.7 + Math.random() * 0.6) : undefined,
        destroyTime: baselineMetrics.destroyTime ? baselineMetrics.destroyTime * (0.7 + Math.random() * 0.6) : undefined,
        memoryUsage: baselineMetrics.memoryUsage ? baselineMetrics.memoryUsage * (0.7 + Math.random() * 0.6) : undefined,
        reRenders: baselineMetrics.reRenders,
        errorCount: baselineMetrics.errorCount ? Math.max(0, baselineMetrics.errorCount - 1) : undefined,
        apiCallCount: baselineMetrics.apiCallCount,
        unusedState: [],
        unusedProps: []
      };
      
      // Calculate improvement percentages
      const improvement = {
        initTime: baselineMetrics.initTime && comparisonMetrics.initTime ? 
          ((baselineMetrics.initTime - comparisonMetrics.initTime) / baselineMetrics.initTime) * 100 : undefined,
        renderTime: baselineMetrics.renderTime && comparisonMetrics.renderTime ? 
          ((baselineMetrics.renderTime - comparisonMetrics.renderTime) / baselineMetrics.renderTime) * 100 : undefined,
        destroyTime: baselineMetrics.destroyTime && comparisonMetrics.destroyTime ? 
          ((baselineMetrics.destroyTime - comparisonMetrics.destroyTime) / baselineMetrics.destroyTime) * 100 : undefined,
        memoryUsage: baselineMetrics.memoryUsage && comparisonMetrics.memoryUsage ? 
          ((baselineMetrics.memoryUsage - comparisonMetrics.memoryUsage) / baselineMetrics.memoryUsage) * 100 : undefined
      };
      
      const result: ModelComparisonResult = {
        pluginId: analysis.pluginId,
        modelType,
        baselineMetrics,
        comparisonMetrics,
        improvement,
        timestamp: new Date()
      };
      
      comparisonResults.push(result);
      
      console.log(chalk.green(`Compared ${analysis.metadata.name || analysis.pluginId}:`));
      
      if (improvement.initTime) {
        const color = improvement.initTime > 0 ? chalk.green : chalk.red;
        console.log(`  Init time: ${color(`${Math.abs(improvement.initTime).toFixed(1)}% ${improvement.initTime > 0 ? 'faster' : 'slower'}`)}`);
      }
      
      if (improvement.renderTime) {
        const color = improvement.renderTime > 0 ? chalk.green : chalk.red;
        console.log(`  Render time: ${color(`${Math.abs(improvement.renderTime).toFixed(1)}% ${improvement.renderTime > 0 ? 'faster' : 'slower'}`)}`);
      }
    }
    
    // Add results to the report
    this.report.modelComparisons = comparisonResults;
    
    // Log comparison results to database
    if (this.options.logToDatabase) {
      await this.logComparisonToDatabase(comparisonResults);
    }
    
    return comparisonResults;
  }

  /**
   * Log analysis report to database
   */
  private async logReportToDatabase(report: EnhancementReport): Promise<void> {
    try {
      // Log the main report
      const dbReport = await prisma.pluginEnhancerReport.create({
        data: {
          timestamp: report.timestamp,
          pluginCount: report.pluginCount,
          okCount: report.summary.ok,
          warningCount: report.summary.warning,
          errorCount: report.summary.error,
          globalSuggestions: {
            create: report.globalSuggestions.map(suggestion => ({
              type: suggestion.type,
              severity: suggestion.severity,
              message: suggestion.message,
              details: suggestion.details || null
            }))
          }
        }
      });
      
      // Log individual plugin analyses
      for (const analysis of report.analyses) {
        await prisma.pluginAnalysis.create({
          data: {
            reportId: dbReport.id,
            pluginId: analysis.pluginId,
            pluginName: analysis.metadata.name || null,
            status: analysis.status,
            initTime: analysis.metrics.initTime || null,
            renderTime: analysis.metrics.renderTime || null,
            destroyTime: analysis.metrics.destroyTime || null,
            errorCount: analysis.metrics.errorCount || null,
            suggestions: {
              create: analysis.suggestions.map(suggestion => ({
                type: suggestion.type,
                severity: suggestion.severity,
                message: suggestion.message,
                details: suggestion.details || null,
                code: suggestion.code || null
              }))
            }
          }
        });
      }
      
      console.log(chalk.green(`Report logged to database with ID: ${dbReport.id}`));
    } catch (error) {
      console.error(chalk.red('Error logging report to database:'), error);
    }
  }

  /**
   * Log model comparison results to database
   */
  private async logComparisonToDatabase(comparisons: ModelComparisonResult[]): Promise<void> {
    try {
      for (const comparison of comparisons) {
        await prisma.modelComparison.create({
          data: {
            pluginId: comparison.pluginId,
            modelType: comparison.modelType,
            baselineInitTime: comparison.baselineMetrics.initTime || null,
            baselineRenderTime: comparison.baselineMetrics.renderTime || null,
            baselineDestroyTime: comparison.baselineMetrics.destroyTime || null,
            comparisonInitTime: comparison.comparisonMetrics.initTime || null,
            comparisonRenderTime: comparison.comparisonMetrics.renderTime || null,
            comparisonDestroyTime: comparison.comparisonMetrics.destroyTime || null,
            initTimeImprovement: comparison.improvement.initTime || null,
            renderTimeImprovement: comparison.improvement.renderTime || null,
            destroyTimeImprovement: comparison.improvement.destroyTime || null,
            timestamp: comparison.timestamp
          }
        });
      }
      
      console.log(chalk.green(`${comparisons.length} model comparison results logged to database`));
    } catch (error) {
      console.error(chalk.red('Error logging comparisons to database:'), error);
    }
  }

  /**
   * Retrieve historical training data from database
   */
  public async getHistoricalTrainingData(): Promise<any> {
    try {
      const reports = await prisma.pluginEnhancerReport.findMany({
        include: {
          globalSuggestions: true
        },
        orderBy: {
          timestamp: 'desc'
        },
        take: 10
      });
      
      const analyses = await prisma.pluginAnalysis.findMany({
        where: {
          reportId: {
            in: reports.map(r => r.id)
          }
        },
        include: {
          suggestions: true
        }
      });
      
      const comparisons = await prisma.modelComparison.findMany({
        orderBy: {
          timestamp: 'desc'
        },
        take: 50
      });
      
      return {
        reports,
        analyses,
        comparisons
      };
    } catch (error) {
      console.error(chalk.red('Error retrieving historical data:'), error);
      return null;
    }
  }
}

// Export classes and interfaces
export { PluginEnhancer }; 